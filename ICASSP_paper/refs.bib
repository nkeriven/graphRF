@InProceedings{C2,
  author = 	 "Jones, C.D. and Smith, A.B. and Roberts, E.F.",
  title =        "Article Title",
  booktitle =        "Proceedings Title",
  organization = "IEEE",
  year = 	 "2003",
  volume = 	 "II",
  pages = 	 "803-806"
}
@article{MMD,
	title={Generative models and model criticism via optimized maximum mean discrepancy},
	author={Sutherland, Dougal J and Tung, Hsiao-Yu and Strathmann, Heiko and De, Soumyajit and Ramdas, Aaditya and Smola, Alex and Gretton, Arthur},
	journal={arXiv preprint arXiv:1611.04488},
	year={2016}
}

@inproceedings{graph_soc_net,
  title={Deep graph kernels},
  author={Yanardag, Pinar and Vishwanathan, SVN},
  booktitle={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={1365--1374},
  year={2015}
}
@article{node_features,
  title={On Node Features for Graph Neural Networks},
  author={Duong, Chi Thang and Hoang, Thanh Dat and Dang, Ha The Hien and Nguyen, Quoc Viet Hung and Aberer, Karl},
  journal={arXiv preprint arXiv:1911.08795},
  year={2019}
}
@inproceedings{RF_1,
  title={Learning kernels with random features},
  author={Sinha, Aman and Duchi, John C},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1298--1306},
  year={2016}
}
@article{class_Reddit,
  title={TUDataset: A collection of benchmark datasets for learning with graphs},
  author={Morris, Christopher and Kriege, Nils M and Bause, Franka and Kersting, Kristian and Mutzel, Petra and Neumann, Marion},
  journal={arXiv preprint arXiv:2007.08663},
  year={2020}
}
@inproceedings{protein_application,
  title={Matching node embeddings for graph similarity},
  author={Nikolentzos, Giannis and Meladianos, Polykarpos and Vazirgiannis, Michalis},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}
@book{isomorphism,
  title={The graph isomorphism problem: its structural complexity},
  author={Kobler, Johannes and Sch{\"o}ning, Uwe and Tor{\'a}n, Jacobo},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@inproceedings{leskovec2006sampling,
  title={Sampling from large graphs},
  author={Leskovec, Jure and Faloutsos, Christos},
  booktitle={Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={631--636},
  year={2006}
}


@article{svm_tut,
  title={Tutorial on support vector machine (svm)},
  author={Jakkula, Vikramaditya},
  journal={School of EECS, Washington State University},
  volume={37},
  year={2006}
}


@article{kriege_graph_kernels,
  title={A survey on graph kernels},
  author={Kriege, Nils M and Johansson, Fredrik D and Morris, Christopher},
  journal={Applied Network Science},
  volume={5},
  number={1},
  pages={1--42},
  year={2020},
  publisher={Springer}
}

@article{GCN_powerful,
  title={How powerful are graph neural networks?},
  author={Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  journal={arXiv preprint arXiv:1810.00826},
  year={2018}
}


@article{frequent_subgraphs,
  title={gSpan: Graph-based substructure pattern mining, 2002},
  author={Yan, X and Han, J},
  journal={Published by the IEEE Computer Society},
  year={2003}
}

@inproceedings{graphlet_kernel,
  title={Efficient graphlet kernels for large graph comparison},
  author={Shervashidze, Nino and Vishwanathan, SVN and Petri, Tobias and Mehlhorn, Kurt and Borgwardt, Karsten},
  booktitle={Artificial Intelligence and Statistics},
  pages={488--495},
  year={2009}
}

@article{isomorphism_np,
  title={Some NP-complete problems similar to graph isomorphism},
  author={Lubiw, Anna},
  journal={SIAM Journal on Computing},
  volume={10},
  number={1},
  pages={11--21},
  year={1981},
  publisher={SIAM}
}
@inproceedings{rahimi2008random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1177--1184},
  year={2008}
}
@article{SBM,
  title={Stochastic block models: A comparison of variants and inference methods},
  author={Funke, Thorben and Becker, Till},
  journal={PloS one},
  volume={14},
  number={4},
  year={2019},
  publisher={Public Library of Science}
}
@article{DD_ref,
  title={Distinguishing enzyme structures from non-enzymes without alignments},
  author={Dobson, Paul D and Doig, Andrew J},
  journal={Journal of molecular biology},
  volume={330},
  number={4},
  pages={771--783},
  year={2003},
  publisher={Elsevier}
}

% NK ref

@inproceedings{oeis,
author={OEIS Foundation Inc.},
year={2019},
title={The online Encyclopedia of Integer Sequences, https://oeis.org/A000088}
}

@article{Bronstein2017,
abstract = {Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
archivePrefix = {arXiv},
arxivId = {1611.08097},
author = {Bronstein, Michael M. and Bruna, Joan and Lecun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
doi = {10.1109/MSP.2017.2693418},
eprint = {1611.08097},
file = {:home/kerivenn/Documents/Mendeley Desktop/2017/Bronstein et al. - 2017 - Geometric Deep Learning Going beyond Euclidean data.pdf:pdf},
isbn = {9781450345385},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
number = {4},
pages = {18--42},
pmid = {21092268},
title = {{Geometric Deep Learning: Going beyond Euclidean data}},
volume = {34},
year = {2017}
}
@inproceedings{Bruna2013,
abstract = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
archivePrefix = {arXiv},
arxivId = {1312.6203},
author = {Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
booktitle = {ICLR},
eprint = {1312.6203},
file = {:home/kerivenn/Documents/Mendeley Desktop/2014/Bruna et al. - 2014 - Spectral Networks and Locally Connected Networks on Graphs.pdf:pdf},
pages = {1--14},
title = {{Spectral Networks and Locally Connected Networks on Graphs}},
url = {http://arxiv.org/abs/1312.6203},
year = {2014}
}
@inproceedings{Gretton2007,
abstract = {We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
archivePrefix = {arXiv},
arxivId = {0805.2368},
author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Sch{\"{o}}lkopf, Bernhard and Smola, Alexander J.},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
eprint = {0805.2368},
file = {:home/kerivenn/Documents/Mendeley Desktop/2007/Gretton et al. - 2007 - A Kernel Method for the Two-Sample Problem.pdf:pdf},
isbn = {0-262-19568-2},
issn = {1049-5258},
pages = {513--520},
title = {{A Kernel Method for the Two-Sample Problem}},
year = {2007}
}
@article{Keriven2017a,
archivePrefix = {arXiv},
arxivId = {1606.02838},
author = {Keriven, Nicolas and Bourrier, Anthony and Gribonval, R{\'{e}}mi and P{\'{e}}r{\`{e}}z, Patrick},
doi = {10.1109/ICASSP.2016.7472867},
eprint = {1606.02838},
file = {:home/kerivenn/Documents/Mendeley Desktop/2018/Keriven et al. - 2018 - Sketching for Large-Scale Learning of Mixture Models.pdf:pdf},
isbn = {2011277906},
issn = {15206149},
journal = {Information and Inference: A Journal of the IMA},
number = {3},
pages = {447--508},
title = {{Sketching for Large-Scale Learning of Mixture Models}},
volume = {7},
year = {2018}
}
@article{Keriven2018,
abstract = {We consider the problem of detecting abrupt changes in the distribution of a multi-dimensional time series, with limited computing power and memory. In this paper, we propose a new method for model-free online change-point detection that relies only on fast and light recursive statistics, inspired by the classical Exponential Weighted Moving Average algorithm (EWMA). The proposed idea is to compute two EWMA statistics on the stream of data with different forgetting factors, and to compare them. By doing so, we show that we implicitly compare recent samples with older ones, without the need to explicitly store them. Additionally, we leverage Random Features to efficiently use the Maximum Mean Discrepancy as a distance between distributions. We show that our method is orders of magnitude faster than usual non-parametric methods for a given accuracy.},
archivePrefix = {arXiv},
arxivId = {1805.08061},
author = {Keriven, Nicolas and Garreau, Damien and Poli, Iacopo},
eprint = {1805.08061},
file = {:home/kerivenn/Documents/Mendeley Desktop/2018/Keriven, Garreau, Poli - 2018 - NEWMA a new method for scalable model-free online change-point detection(2).pdf:pdf},
journal = {arXiv:1805.08061},
pages = {1--22},
title = {{NEWMA: a new method for scalable model-free online change-point detection}},
year = {2018}
}
@article{Sriperumbudur2010,
author = {Sriperumbudur, Bharath K. and Gretton, Arthur and Fukumizu, Kenji and Sch{\"{o}}lkopf, Bernhard and Lanckriet, Gert R.G.},
file = {:home/kerivenn/Documents/Mendeley Desktop/2010/Sriperumbudur et al. - 2010 - Hilbert space embeddings and metrics on probability measures.pdf:pdf},
journal = {The Journal of Machine Learning Research},
pages = {1517--1561},
title = {{Hilbert space embeddings and metrics on probability measures}},
volume = {11},
year = {2010}
}
@article{Dutta2018,
abstract = {Graph-based methods are known to be successful in many machine learning and pattern classification tasks. These methods consider semistructured data as graphs where nodes correspond to primitives (parts, interest points, and segments) and edges characterize the relationships between these primitives. However, these nonvectorial graph data cannot be straightforwardly plugged into off-the-shelf machine learning algorithms without a preliminary step of--explicit/implicit--graph vectorization and embedding. This embedding process should be resilient to intraclass graph variations while being highly discriminant. In this paper, we propose a novel high-order stochastic graphlet embedding that maps graphs into vector spaces. Our main contribution includes a new stochastic search procedure that efficiently parses a given graph and extracts/samples unlimitedly high-order graphlets. We consider these graphlets, with increasing orders, to model local primitives as well as their increasingly complex interactions. In order to build our graph representation, we measure the distribution of these graphlets into a given graph, using particular hash functions that efficiently assign sampled graphlets into isomorphic sets with a very low probability of collision. When combined with maximum margin classifiers, these graphlet-based representations have a positive impact on the performance of pattern comparison and recognition as corroborated through extensive experiments using standard benchmark databases.},
archivePrefix = {arXiv},
arxivId = {1702.00156},
author = {Dutta, Anjan and Sahbi, Hichem},
doi = {10.1109/TNNLS.2018.2884700},
eprint = {1702.00156},
file = {:home/kerivenn/Documents/Mendeley Desktop/2018/Dutta, Sahbi - 2018 - Stochastic Graphlet Embedding.pdf:pdf},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Betweenness centrality,graph classification,graph embedding,graph hashing,stochastic graphlets.},
title = {{Stochastic Graphlet Embedding}},
year = {2018}
}


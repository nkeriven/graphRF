% Template for ICASSP-2020 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
%
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{subfig}
\usepackage[labelfont=bf, justification=justified]{caption}


\usepackage{spconf,amsmath,graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{ifpdf}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{bm}
\usepackage[toc,page]{appendix}
\usepackage{hyperref}
%\usepackage{cite}
% !TeX spellcheck = en_US 
% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{Fast graph kernel with optical random features}
%
% Single address.
% ---------------
\name{ \qquad Hashem Ghanem\qquad Nicolas Keriven \qquad Nicolas Tremblay }

\address{ CNRS, GIPSA-lab, FR-38402 Saint Martin dâ€™Heres Cedex, France}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}


\newcommand\nt[1]{\textcolor{blue}{#1}}
%
\begin{document}
%\ninept
%
\newtheorem{theorem}{Theorem} 
\maketitle
%
\begin{abstract}
%Graph Classification is the problem of mapping graphs to the classes they belong to.
The graphlet kernel is a classical method in graph classification. It however suffers from a high computation cost due to the isomorphism test it includes. As a generic proxy, and in general at the cost of losing some information, this test can be efficiently replaced by a user-defined mapping that computes various graph characteristics. In this paper, we propose to leverage \emph{kernel random features} within the graphlet framework, and establish a theoretical link with a mean kernel metric. If this method can still be prohibitively costly for usual random features, we then incorporate \emph{optical} random features that can be computed in \emph{constant time}. Experiments show that the resulting algorithm is orders of magnitude faster that the graphlet kernel for the same, or better, accuracy.
\end{abstract}
%
\begin{keywords}
Optical random features, Graph kernels
\end{keywords}
%
\section{Introduction}
\label{sec:intro}
In mathematics and data science, graphs are used to model a set of objects (the \emph{nodes}) and their interactions (the \emph{edges}). %Having \emph{a priori} known graph classes, \emph{graph classification} consists in predicting the class of an entire graph.
Given a set of pre-labeled graphs $(\mathcal{X}=\{\G_1,\ldots,\G_n\}, \mathcal{Y}=\{y_1,\ldots,y_n\})$, where each graph $\G_i$ belongs to the class with label $y_i$, graph classification consists in designing an algorithm that outputs the class label of a new graph.
%
For instance, proteins can be modeled as graphs: amino acids are nodes and the chemical links between them are edges. They can be classified to enzymes and non-enzymes \cite{protein_application}.
%
In social networks analysis, post threads can be modeled with graphs whose nodes are users and edges are replies to others' comment \cite{graph_soc_net}. One task is then to discriminate between discussion-based and question/answer-based threads \cite{class_Reddit}.
% where an edge between two nodes exists if one replies to the other's comment on that thread 
 %For the two examples, , as in the D\&D dataset. In social networks, , as in Reddit-Binary dataset.
%
In addition to the graph structure, nodes and edges may have extra features. While it has been shown that node features are important to obtain high classification performance \cite{node_features}, here we focus on the case where one has only access to the graph structure.%\BlankLine


%\noindent\textbf{Related work.}
Structure-based graph classification has been tackled with many algorithms. Frequent subgraphs based algorithms \cite{frequent_subgraphs} analyze the graph dataset $\mathcal{X}$ to catch the frequent and discriminative subgraphs and use them as features. Kernel-based algorithms \cite{kriege_graph_kernels} can be used by defining similarity functions (kernels) between graphs. An early and popular example is the \emph{graphlet kernel}, which computes frequencies of subgraphs. It is however known to be quite costly to compute \cite{graphlet_kernel}, in particular due to the presence of graph isomorphism tests. While possible in particular cases \cite{graphlet_kernel}, accelerating the graphlet kernel for arbitrary datasets remains open.
Finally, graph neural networks (GNNs) \cite{Bruna2013, Bronstein2017} have recently become very popular in graph machine learning. They are however known to exhibit limited performance when node features are unavailable \cite{GCN_powerful}. %Recently, a particular model called GIN (Graph Isomorphism Network) was developed and provided high performance classification.
%\BlankLine

%\noindent\textbf{Contribution:}
In kernel methods, random features are an efficient approximation method~\cite{rahimi2008random, RF_1}. Recently, it has been shown \cite{saade_opu} that \emph{optical computing} can be leveraged to compute such random features in \emph{constant time} in \emph{any dimension} -- within the limitations of the current hardware, here referred to as Optical Processing Units (OPUs).
%On the other hand, the graphlet kernel represents a graph by how many times graphs of a smaller size occur in it. This kernel includes the isomorphism test in the counting process which makes it computationally expensive.
The main goal of this paper is to provide a proof-of-concept answer to the following question: can OPU computations be used to reduce the computational complexity of a combinatorial problem like the graphlet kernel? Drawing on a connection with mean kernels and Maximum Mean Discrepancy (MMD) \cite{Gretton2007}, we show, empirically and theoretically, that a fast and efficient graph classifier can indeed be obtained with OPU computations.

\section{Background}
\label{sec:background}
%\subsection{The graphlet kernel}\label{sec:graphlet_kernel}
First, we present the concepts necessary to define the graphlet kernel. We represent a graph of size $v$ by the adjacency matrix $\mathbf{A}\in \{0,1\}^{v\times v}$, such that $a_{i,j} =1$ if there is an edge between nodes $\{i,j\}$ and $0$ otherwise. Two graphs are said to be isomorphic ($\G\cong \G')$ if we can permute the nodes' labels of one such that their adjacency matrices are equal \cite{isomorphism}. 

\subsection{Isomorphic graphlets}

In this paper, we will, depending on the context, manipulate two different notions of $k$-graphlets (that is, small graphs of size $k$): with or without discriminating isomorphic graphlets. We denote by $\bar{\mathfrak{H}}=\{\bar{\phlet}_1,..., \bar{\phlet}_{\bar N_k}\}$ with $\bar{N}_k = 2^{\frac{k(k-1)}{2}}$ the set of all size-$k$ graphs, where isomorphic graphs are counted multiple times, and $\mathfrak{H}=\{\phlet_1,..., \phlet_{N_k}\} \subset \bar{\mathfrak{H}}$ the set of all non-isomorphic graphs of size $k$. Its size $N_k$ has a (quite verbose) closed-from expression \cite{oeis}, but is still exponential in $k$.
%
In the course of this paper, we shall manipulate mappings $\varphi(\mathcal{H})$ and probability distributions (histograms) over graphlets. By default the underlying space will be $\bar{\mathfrak{H}}$, however when the mapping $\varphi$ is \emph{permutation-invariant}, then the underlying space can be thought of as $\mathfrak{H}$. %Using one or the other obeys a tradeoff: $\bar{\mathfrak{H}}$ is larger, while $\mathfrak{H}$ may necessitate costly isomorphism tests. 
Also note that, assuming each isomorphic copies has equal probability, a probability distribution over $\bar{\mathfrak{H}}$ can be \emph{folded} into one over $\mathfrak{H}$, and both distributions \emph{contain the same amount of information}.

\subsection{The graphlet kernel}

The traditional graphlet kernel is defined by computing histograms of subgraphs over non-isomorphic graphlets $\mathfrak{H}$. We define the matching function $\varphi_k^{match}(\mathcal{F}) = \left[ 1_{(\mathcal{F} \cong \phlet_i)}\right]_{i=1}^{N_k} \in \{0,1\}^{N_k}$, where $\mathcal{F}$ is a graph of size $k$. In words, $\match(\mathcal{F})$ is a one-hot vector of dimension $N_k$ identifying $\mathcal{F}$ up to isomorphism. Note that the cost of evaluating $\varphi_k^{match}$ once is $O\left(N_k C^{\cong}_k\right)$, where $C^{\cong}_k$ is the cost of the isomorphism test between two graphs of size $k$, for which no polynomial algorithm is known \cite{isomorphism_np}. 
%
Given a graph $\G$ of size $v$, let $\mathfrak{F}_\G=\{\mathcal{F}_1,\mathcal{F}_2,\ldots,\mathcal{F}_{\binom{v}{k}}\}$ be the collection of subgraphs induced by all size-$k$ subsets of nodes. The following representation vector is called the $k$-spectrum of $\G$:
\begin{equation}
\label{eq:gk}
\mathsmaller{\mathbf{f}_\G=\binom{v}{k}^{-1}\sum_{\mathcal{F}\in\mathfrak{F}_\mathcal{G}} \match (\mathcal{F}) \in \R^{N_k}}
\end{equation}
For two graphs $\G,\G'$, the graphlet kernel \cite{graphlet_kernel} is then defined as $\bld{f}_\G^T\bld{f}_{\G'}$. %The graphlet kernel performs well especially with a sufficiently large value of $k$ \cite{graphlet_kernel}. However, in each graph $\G$ of size $v$,
For a graph of size $v$, the computation cost of $\mathbf{f}_\G$ is $C_{gk}= \mathcal{O}\left(\tbinom{v}{k}N_k C^{\cong}_k\right)$. This cost is usually prohibitively expensive, since each three terms are exponential in $k$.

Subgraph sampling is generally used as a first step to accelerate (and sometimes modify) the graphlet kernel \cite{graphlet_kernel}. Given a graph $\G$, we denote by $S_k(\G)$ a sampling process that yields a random subgraph of $\G$, seen as a probability distribution over $\bar{\mathfrak{H}}$. %There exists many variants \cite{lesco}. 
Then, sampling $s$ subgraphs $\hat{\mathfrak{F}}_\G = \{F_1,...,F_s\}$ $i.i.d.$ from $S_k(\G)$, we define the estimator:
\begin{align}
	\label{eq:fhat_unif}
	\mathsmaller{\hat{\mathbf{f}}_{\mathcal{G},S_k} =s^{-1}\sum_{F\in\hat{\mathfrak{F}}_\G} \varphi^{match}_k(F).}
\end{align}
and its expectation $\mathbf{f}_{\mathcal{G},S} = \mathbb{E}_{F \sim S_k(\G)} ~\varphi^{match}_k(F)$, which is nothing more than the \emph{folding} of the distribution $S_k(\G)$ over non-isomorphic graphlets $\mathfrak{H}$. For any sampler, we refer to these expectations as graphlet kernels. In fact, in all generality, any choice of sampling procedure $S_k$ yields a different definition of graphlet kernel. For instance, if one considers uniform sampling ($S^{\rm unif}$: independently samples $k$ nodes of $\G$ without replacement), then one obtains the original graphlet kernel of Eq.~\eqref{eq:gk}: $\mathbf{f}_{\G, S^{\rm unif}} = \mathbf{f}_\G$. Other choices of sampling procedures are possible~\cite{leskovec2006sampling}. In this paper, we will also use the random walk (RW) sampler, which, unlike uniform sampling, tends to sample connected subgraphs, which may be more informative about the graph structure. 

The computation cost per graph of the approximate graphlet kernel of Eq.~\eqref{eq:fhat_unif} is $C_{gk + gs}= \mathcal{O}\left(s C_S N_k C^{\cong}_k\right)$, where $C_S$ is the cost of sampling one subgraph. %Although the term $\binom{v}{k}$ doesn't exist  in this cost, still 
For a fixed error in estimating $\bld{f}_{\G,S}$, the required number of samples $s$ generally needs to be proportional to $N_k$ \cite{graphlet_kernel}, which unfortunately still yields an unaffordable algorithm.

%where sampling a size-k subgraph means: first we uniformly at random choose $k$ nodes from the graph, then we sample the subgraph induced by those  nodes. This yields that each sample follow a uniform distribution over $\mathfrak{F}_\G$.

%Knowing this, the $k$-spectrum $\bld{f}_\G$ can be interpreted as follows: if one samples a subgraph from $\mathcal{G}$, then one has a probability $(\bld{f}_\G)_i$ of obtaining $\phlet_i$, \emph{i.e.}: $	\label{eq:histo_unif}
%	\mathbf{f}_\mathcal{G} = \mathbb{E}_{F \sim {\rm unif}} ~\varphi^{match}_k(F)$. 
%	It is thus natural to approach $\mathbf{f}_\mathcal{G}$ with
%verifies by the law of large numbers that $\hat{\mathbf{f}}_\G \xrightarrow[s \to \infty]{} \mathbf{f}_\mathcal{G}$ with probability $1$.


%\iffalse
%\subsection{Kernel Random features}\label{sec:Random_features}
%Kernels by definition are symmetric and positive  semi-definite functions that takes two data points as input. Based on Mercer theorem, for each kernel $\kappa$, there exists a Hilbert space $\mathbb{H}$ and a  feature map $\phi:\mathbb{R}^d\mapsto\mathbb{H}$ such that:  
%	\begin{equation}
%	\label{eq:kernel_main_equation}
%	\kappa(\mathbf{x},\mathbf{x}')=<\phi(\mathbf{x}),\phi(\mathbf{x}')>_\mathbb{H},~ \forall \mathbf{x},\mathbf{x}'\in\mathbb{R}^d
%	\end{equation}
%	where $<\phi(\mathbf{x}),\phi(\mathbf{x}')>_\mathbb{H}$ is the inner product defined in $\mathbb{H}$.
%
%Random features (RF) is an approach developed to approximate kernels with reduced computational time  \cite{rahimi2008random}. The idea is that, instead of considering the true lifting function $\phi$ in Eq. \ref{eq:kernel_main_equation}, we explicitly map the data points using an appropriate randomized feature map $\varphi:\mathbb{R}^d \xrightarrow{}\mathbb{C}^m$, such that the kernel for two data points $\mathbf{x}, \mathbf{x}'$ is approximated by the inner product of their random features with high probability:
%\begin{equation}
%\label{eq:approx_RF}
%\kappa(\mathbf{x},\mathbf{x}')=<\phi(\mathbf{x}),\phi(\mathbf{x}')>_\mathbb{H} \approx \varphi(\mathbf{x})^*\varphi(\mathbf{x}')
%\end{equation}
%where $^*$ stands for the conjugate transpose. Most RF constructions are known as Random Fourier Features (RFF), and are based on
%the following theorem.
%\begin{theorem}[Bochner's theorem]
%	A continuous and shift-invariant kernel $\kappa(\mathbf{x},\mathbf{x}')=\kappa(\mathbf{x}-\mathbf{x}')$ on $\mathbb{R}^d$ is positive definite if and only if $\kappa$ is the Fourier transform of a non-negative measure.
%\end{theorem}
%Therefore, scaling such kernels to obtain $\kappa(0) = \int p = 1$,   its Fourier transform $p(\mathbf{w})$ becomes a correct probability distribution, so we write:
%\begin{equation}
%\label{eq:real_Fourier_integral}
%\kappa(\mathbf{x}-\mathbf{x}')=\int_{\mathbb{R}^d}p(\mathbf{w})cos({\mathbf{w}^T(\mathbf{x}-\mathbf{x}')})d\mathbf{w}=E_{\mathbf{w}\sim p}[ \xi_\mathbf{w}(\mathbf{x}) \xi_\mathbf{w}(\mathbf{x}')]
%\end{equation}
%where $ \xi_\mathbf{w}(\mathbf{x})=\sqrt{2}cos(\mathbf{w}^T\mathbf{x}+b)$ such that $\mathbf{w}$ is drawn from $p$ and $b$ is drawn uniformly from $[0,2\pi]$. The RF methodology consists in averaging $m$ instances of $\  \xi_\mathbf{w}(\mathbf{x})^*  \xi_\mathbf{w}(\mathbf{x}')$  with different random frequencies $\mathbf{w}_j$ drawn identically and independently (iid) from $p$, that is, define
%\begin{align}
%	\label{eq:def_RF}
%	\varphi(\mathbf{x}) = \frac{1}{\sqrt{m}} ( \xi_{\mathbf{w}_j}(\mathbf{x}) )_{j=1}^m \in \mathbb{C}^m
%\end{align}
%such that $\varphi(\mathbf{x})^*\varphi(\mathbf{x}')=\frac{1}{m} \sum_{j=1}^m \xi_{\mathbf{w}_j}(\mathbf{x})^*\xi_{\mathbf{w}_j}(\mathbf{x}')$, which, by Hoeffding's inequality, converges exponentially in $m$ to $\kappa(\mathbf{x},\mathbf{x}')$.
%\fi

\begin{algorithm}
	\label{alg:gsa}
	\DontPrintSemicolon
	\KwInput{labeled graph dataset $\mathcal{X}=(\G_i,y_i)_{i=1,\ldots,n}$}
	\tools{Graphlet sampler $S_k$, a function $\varphi$, a linear classifier (ex. SVM) }\\
	\Hyp{k: graphlet size, $s$: number of graphlet samples}, $m$: \emph{number of random features}\\
	\KwOutput{Trained model to classify graphs}
	%\KwData{Testing set $x$}
	%$\sum_{i=1}^{\infty} := 0$ \tcp*{this is a comment}
	%\tcc{}
	\Algo{\\}
	Random initialization of the SVM weights\\
	\For{$\G_i$ in $\mathcal{X}$}{
		$\mathbf{z}_i=\mathbf{0}$ (null vector of size $m$) \\
		\For{$j=1:s$}{
			$F_{i,j}\gets S_k(\G_i)$\\
			$\mathbf{z}_i\gets \mathbf{z}_i +\frac{1}{s}\varphi(F_{i,j})$
		}
	}
	$\mathcal{D}_{\varphi}\gets (\mathbf{z}_i,y_i)_{i=1,\ldots, n}$\\
	Train the classifier on this vector-valued dataset $\mathcal{D}_{\varphi}$
	\caption{GSA-$\varphi$ generic algorithm}
\end{algorithm}

\section{Graphlet kernel with optical maps} \label{ssed to get a lowerec:pagestyle}
\subsection{Proposed method}
\label{sec:algo}
In this paper, we focus on the main remaining bottleneck of the graphlet kernel, that is, the function $\match$. We define a framework where it is replaced with a user-defined map $\varphi: \bar{\mathfrak{H}} \to \mathbb{R}^m$, which leads to the final representation:
\begin{align}
	\label{eq:fhat_phi}
	\mathsmaller{\hat{\mathbf{f}}_{\mathcal{G},S_k,\varphi} =s^{-1}\sum_{F\in\hat{\mathfrak{F}}_\G} \varphi(F).}
\end{align}
and similarly its expectation $\mathbf{f}_{\mathcal{G},S_k,\varphi}$.
%, and to replace the uniform sampler with a user-chosen one like the RW sampler.  
%This makes it possible to deploy the OPU technology with this algorithm. 
The resulting methodology is referred to as \emph{Graphlet Sampling and Averaging} (GSA-$\varphi$), and summarized in Alg.~\ref{alg:gsa}. The cost of computing \eqref{eq:fhat_phi} is $C_{GSA-\varphi}= \mathcal{O}\left(s C_S C_{\varphi}\right)$, where $C_{\varphi}$ is the cost of applying $\varphi$.
%
%The function $\varphi$ here maps each subgraph to an $m$-dimensional space $\mathbb{R}^m$.
%Note that choosing $\varphi = \match$ and $S_k$ as the uniform sampler, GSA-$\match$ turns out to be the graphlet kernel with sampling.
%
Similar methods have been studied with $\varphi$ as simple graphlets statistics \cite{Dutta2018}, which unavoidably incurs information loss. We see next that choosing $\varphi$ as \emph{kernel random maps} preserves information for a sufficient number of random features. Some of these maps will \emph{not} be permutation-invariant at the graphlet level, however, in the infinite sample limit, it is easy to see that the representation vector $\mathbf{f}_{\mathcal{G},S_k,\varphi}$ is indeed permutation-invariant at the graph level.

\subsection{Kernel random features with $GSA-\varphi$} 
\label{sec:MMD}

In the graphlet kernel, the underlying metric used to compare graphs is the Euclidean distance between graphlet histograms. When $\match$ is replaced by another $\varphi$, one compares certain \emph{embeddings} of distributions, which is reminiscent of kernel mean embeddings \cite{Gretton2007}. We show below that this corresponds to choosing $\varphi$ as kernel random features.

For two objects $\mathbf{x}, \mathbf{x'}$, a kernel $\kappa$ associated to a random features (RF) decomposition is a positive definite function that can be decomposed  as follows \cite{rahimi2008random}:
\begin{equation}
\label{eq:RF_decomposition}
\kappa(\mathbf{x},\mathbf{x}')=\mathbb{E}_{\mathbf{w}\sim p}[ \xi_\mathbf{w}(\mathbf{x})^* \xi_\mathbf{w}(\mathbf{x}')]
\end{equation}
where $\xi$ is a real (or complex) function parameterized by $\mathbf{w} \in \Omega$, and $p$ a probability distribution on $\Omega$. A classical example is the Fourier decomposition of translation-invariant kernels~\cite{rahimi2008random}.
%To approximate such kernels, we can empirically average $m$ realizations of $\xi_\mathbf{w}(\mathbf{x})^* \xi_\mathbf{w}(\mathbf{x}')$. 
The RF methodology then defines maps:
\begin{equation}
	\label{eq:def_RF}
	\mathsmaller{
	\varphi(\mathbf{x}) = m^{-1/2} ( \xi_{\mathbf{w}_j}(\mathbf{x}) )_{j=1}^m \in \mathbb{C}^m}
\end{equation}
where $m$ is the number of features and the parameters $\mathbf{w}_j$ are drawn iid from $p$. Then, $\kappa(\mathbf{x},\mathbf{x}')\approx	\varphi(\mathbf{x})^H	\varphi(\mathbf{x}') = m^{-1} \sum_j \xi_{\mathbf{w}_j}(\mathbf{x})^* \xi_{\mathbf{w}_j}(\mathbf{x}')$. 

Assume that we have a base kernel $\kappa(\F,\F')$ between \emph{graphlets}, with a RF decomposition $(\xi_\mathbf{w}, p)$, and define $\varphi$ as in \eqref{eq:def_RF}. Then, one can show \cite{Keriven2017a, Keriven2018} that the Euclidean distance between the embeddings \eqref{eq:fhat_phi} approximates the following \emph{Maximum Mean Discrepancy} (MMD) \cite{Gretton2007, Sriperumbudur2010} between distributions on $\bar{\mathfrak{H}}$:
 \begin{multline}\label{eq:MMD}
 \mathsmaller{
 \textup{MMD}^2(S_k(\G), S_k(\G'))} \\
 \mathsmaller{= \mathbb{E}_{\mathbf{w}} \Big( \left| \mathbb{E}_{S_k(\G)} \xi_\mathbf{w}(F) - \mathbb{E}_{S_k(\G')} \xi_\mathbf{w}(F') \right|^2 \Big)}
 \end{multline}
The main property of the MMD is that, for so-called \emph{characteristic kernels}, it is a true metric on distributions, \emph{i.e.} $\textup{MMD}(S_k(\G), S_k(\G')) = 0 \Leftrightarrow S_k(\G) = S_k(\G')$. % that is, it is $0$ if and only if $S_k(\G)$ and $S_k(\G')$ incur the same distribution over $\bar{\mathfrak{H}}$. 
Most usual kernels, like the Gaussian kernel, are characteristic \cite{Gretton2007, Sriperumbudur2010}.
%
%It states  that the Euclidean distance between the representation vectors $\bld{z}_\G, \bld{z}_{\G'}$ converges to the MMD metric between their distributions $\bld{f}_{\G,S_k}$, $\bld{f}_{\G',S_k}$. 

%\textcolor{red}{todo: $S_k$ as a distribution over non-isomorphic graphlets. Isomorphism is a special case}

\begin{theorem}\label{theorem:concentration}
Let $\G$ and $\G'$ be two graphs. %$\mathfrak{F}_\G=\{F_i\}_{i=1}^{s}$ (resp. $\mathfrak{F}_{\G'}=\{F_i'\}_{i=1}^{s}$) be $iid$ size-k graphlet samples drawn from $S_k(\G)$ (resp. $S_k(\G')$). 
Assume a random feature map as in \eqref{eq:def_RF}. Assume that $|\xi_\mathbf{w}(F)| \leq 1$ for any $\mathbf{w},F$.
We have for all $\delta>0$ and with probability at least $1-\delta$:
\begin{multline}\label{eq:bound}
\mathsmaller{
 \Big|\|\hat{\mathbf{f}}_{\G,S_k,\varphi} - \hat{\mathbf{f}}_{\G',S_k,\varphi} \|_2^2 - \textup{MMD}^2(S_k(\G),S_k(\G')) \Big| } \\
 \mathsmaller{ \leq 4 m^{-\frac12} \sqrt{\log (6/\delta)} + 8s^{-\frac12} \left(1+\sqrt{2\log(3/\delta)}\right)}
\end{multline}
\end{theorem}
\begin{proof}
	See the extended version of this article~\cite{arxiv_version}.
\end{proof}
Hence, if two classes of graphs are well-separated in terms of the MMD \eqref{eq:MMD}, then, for sufficiently large $m,s$, GSA-$\varphi$ has the same classification power. However, according to \eqref{eq:bound}, $m$ should be of the order of $s$, and we have seen that the latter generally needs to be quite large:  most usual random feature scheme, typically in $C_\varphi =O(k^2 m)$, still have a high computation cost. We discuss next the use of \emph{optical hardware}.

\subsection{Considered choices of $\varphi_{RF}$}
\label{sec:phi_choices}
\textbf{Gaussian maps}: the RF map of the Gaussian kernel \cite{rahimi2008random}. 
\begin{equation}
\label{eq:Gaussian_map}
 \mathsmaller{\varphi_{Gs}(\F) = m^{-1/2} \left( \sqrt{2} \cos(\mathbf{w}_j^T\mathbf{a}_\F+b_j) \right)_{j=1}^m \in \mathbb{R}^m}
\end{equation}
where $\mathbf{a}_\F=flatten(\mathbf{A}_\F)$ is the vectorized adjacency matrix of the graphlet $\F$, 
the $\mathbf{w}_j\in \R^{k^2}$ are drawn from a Gaussian distribution and $b_j \sim \mathcal{U}([0, 2\pi])$. While using a Gaussian kernel on $\mathbf{a}_\mathcal{F}$ is not very intuitive, this will serve as a baseline for other methods. Note that $\varphi_{Gs}$ is not permutation-invariant.
\BlankLine
\noindent\textbf{Gaussian maps applied on the sorted eigenvalues:} We consider a permutation-invariant alternative to the first case. %Instead of passing the vectorized adjacency matrix as input, 
For a graphlet $\F$ we denote the vector of its \emph{sorted} eigenvalues by $\bm{\lambda}(\F) \in\R^k$ and $\varphi_{Gs+eig}(\F) = \varphi_{Gs}(\bm{\lambda}(\F))$ (with $\mathbf{w}_j$ of dimension $k$). Note that the existence of co-spectral graphs, that is, non-isomorphic graphs with the same set of eigenvalues, implies a loss of information when computing $\bm{\lambda}(\F)$.
%The motive proposing $\varphi_{Gs+Eig}$ is that it respects the isomorphism test since: 
%i/ $\bld{\lambda}(\bld{A})=\bld{\lambda}(\bld{PAP^T})$ for any permutation matrix $\bld{P}$, 
%ii/ $F\cong F' \Rightarrow\exists \bld{P}, \bld{A}_F=P\bld{A}_{F'}P^T$.  
%Thus, $F\cong F' \Rightarrow \varphi_{Gs+Eig}(F)=\varphi_{Gs+Eig}(F')$. $\varphi_{Gs+Eig}$ maps isomorphic subgraphs to the same point in $\R^m$.
\BlankLine
\noindent\textbf{Optical random feature maps:} Due to high-dimensional matrix multiplication, Gaussian RFs cost $\mathcal{O}(mk^2)$ and are notably expensive to compute in high-dimension (in this case, large $m$). To solve this, OPUs (Optical Processing Units) were recently developed to compute a specific random features mapping in \emph{constant time  $\mathcal{O}(1)$} using light scattering~\cite{saade_opu} -- within the physical limits of the OPU, currently of the order of a few millions for both input and output dimensions. Here we again consider the flattened adjacency matrix for simplicity. The OPU computes an operation of the type:
\[
\label{OPU_equation}
\mathbf{\varphi}_{OPU}(\F)= m^{-1/2}\left(|\mathbf{w}_j^T \mathbf{a}_\F+\mathbf{b}_j|^2\right)_{j=1}^m
\]
with $\mathbf{b}_j$ a random bias and $\mathbf{w}_j$ a complex vector with Gaussian real and imaginary parts. Both $\mathbf{w}_j, \mathbf{b}_j$ are here incurred by the physics and are unknown, however the corresponding kernel $\kappa(\F,\F')$ has a closed-form expression~\cite{saade_opu}.
%
Table \ref{tab:cost} summarizes the complexities of the mappings $\varphi$ examined.

\begin{table}
\centering
\begin{tabular}{|c|c|c|}
\hline
\multicolumn{2}{|c|}{Graphlet kernel} & $O(\tbinom{v}{k} N_k C^{\cong}_k)$\\ \hline \hline
%
\multirow{4}{*}{GSA-$\varphi$ with:} & $\varphi^{match}_k$ & $O(C_S s N_k C^{\cong}_k)$ \\
& $\varphi_{Gs}$ & $O(C_S s m k^2)$ \\ 
& $\varphi_{Gs+eig}$  & $O(C_S s (m k + k^3))$ \\ 
& $\varphi_{OPU}$  & $O(C_S s)$ \\ \hline
\end{tabular}
\caption{Per-graph complexities of GSA-$\varphi$.}
\label{tab:cost}
\end{table}


\section{Experiments}\label{sec:experiments}
\subsection{Datasets}\label{sec:setup}
%With respect to the performance and  the computation time, we compare different choices of  map $\varphi$ in $GSA-\varphi$. We benchmark the performance of $GSA-\varphi_{OPU}$ against GIN based graph convolutional network \cite{GCN_powerful}.
%
In order to first compare performances of different methods in a controlled setting, we consider a synthetic dataset generated by a \emph{Stochastic Block Model (SBM)} \cite{SBM}. We generate $300$ graphs, $240$ for training  and $60$ for testing. Each graph has $v=60$ nodes divided equally in six communities. Moreover, graphs are divided into two classes $\{0 , 1\}$. For each class we fix $p_{in}$ (resp. $p_{out}$) which is the probability of generating an edge between any two nodes in the same (resp. different) community. Besides, to prevent the classes from being easily discriminated by the average degree, the pairs $(p_{in,i} , p_{out,i})_{i=0,1}$ are chosen such that nodes in graphs of both classes have the same expected degree (set to $10$). Having one degree of freedom left, we fix $p_{in,1}$ to $0.3$, and vary $r=(p_{in,1}/p_{in,0})$ the inter-class similarity parameter: the closer $r$ is to $1$, the more similar both classes are, and thus the harder it is to discriminate them.

In addition, we perform experiments on two real-world datasets: D\&D \cite{DD_ref} and Reddit-Binary \cite{class_Reddit} are of size $n=1178$ and  $n=2000$ respectively. We recall that even though D\&D dataset is labeled, the graphs are classified based on their structure only and all other information is discarded. Python codes can be found on \url{https://github.com/hashemghanem/OPU_Graph_Classifier.git}

%In D\&D, nodes have 7 features each, which are not used by our algorithms, \emph{i.e.} we  classify  graphs based on their structure only. 

%In what follows, unless otherwise stated, we use uniform sampling and the adjacency matrix of subgraphs as input. 

\begin{figure}
	\captionsetup[subfigure]{justification=centering}
	\centering
	\subfloat[$\varphi_{OPU}$\& uniform sampling]
	{
		\includegraphics[width=4.3cm]{figs/LightOn_adj_SBM_Similarity_graphlet_size.pdf}%
	}
	%\hspace{1cm}%
	\subfloat[$\varphi_{OPU}$\& RW sampling,\\ graphlet kernel, and GIN model]
	{%
		\includegraphics[width=4.3cm]{figs/LightOn_adj_SBM_similarity_graphlet_size_RW.pdf}%
	}
	\caption{(left)~performance of $GSA-\varphi_{OPU}$ with uniform sampling ($s=2000$) for different values of $k$ ($m$ fixed at $5000$) and $m$ ($k$ fixed to $6$). (right)~comparison of the performance of $GSA-\varphi_{OPU}$ with RW sampling for different values of $k$, versus $GSA-\match$ with same number of samples and random features $s=2000$ and $m=5000$; as well as the GIN-based model consisting of 5 GIN layers followed by 2 fully connected layers, the dimension of hidden layers= 4.\nt{*remove the hard-encoded titles in the pdf as well as the (ab). btw, the curve $m=5000$ is not plotted on the left figure (which makes sense as it should be the same as $k=6$): remove $m=5000$ from the legend.*}
	}
	\label{fig:GCN}
\end{figure}

\subsection{Varying $m, k$ and $S_k$ in $GSA-{\varphi_{OPU}}$}
From Fig. \ref{fig:GCN} (left), we observe that as $k$ and/or $m$ increase, the performance of $GSA-{\varphi_{OPU}}$ associated to uniform sampling increases, saturating in this SBM dataset for $m=5000$ and $k=6$. From the right figure, and as expected, we note that RW sampling provides better results than the uniform version: the smaller $k$, the larger the improvement.
%the difference being  when the subgraph size $k$  is smaller or equal to 4. %Moreover, when increasing $m$ and fixing other parameters, similarly noted for $k$, accuracy curves converge to the best curve that corresponds to $m\rightarrow\infty$.

\subsection{Choice of feature map $\varphi$}
\textbf{Comparison of random features}. Fig \ref{fig:diff_phi} (left) shows  that, for sufficiently large $m$, $GSA-\varphi_{OPU}$  outperforms both $GSA-\varphi_{Gs+Eig}$  and $GSA-\varphi_{Gs}$ (whose variance $\sigma^2$ is chosen so as to maximize the validation accuracy). %Note that  $GSA-\varphi_{Gs+Eig}$ outperforms $GSA-\varphi_{OPU}$ at small $m$, but then stays constant as $m$ increases. A possible justification is that the eigenvalues of the adjacency matrix lose information about the subgraphs, even though respecting the isomorphism means that we are working with a smaller histogram and less random features are required. 

\noindent\textbf{Comparing $GSA-\varphi_{OPU}$ and $GSA-\match$}. From Fig~\ref{fig:GCN} (right) we observe that with $s=2000$ and $m=5000$, $GSA-\varphi_{OPU}$ with both uniform sampling ($k=6$) and RW sampling ($k=5$)  clearly outperforms the graphlet kernel % with graphlet sampling $GSA-\match$ 
with $k=6$. % We conclude that $GSA-\varphi_{OPU}$ is more adapted in this case than the traditional graphlet kernel. 

\noindent\textbf{Computational time}. Fig \ref{fig:diff_phi} (right) compares computation time per subgraph, with respect to the subgraph size $k$. %Other parameters are identically fixed for all methods. 
As expected, the execution time is exponential with $k$ for $GSA-\match$, roughly polynomial for $GSA-\varphi_{Gs}$ and $GSA-\varphi_{Gs+Eig}$, and constant for $GSA-\varphi_{OPU}$.% and significantly the lowest when $k\geq7$. 

To summarize, $GSA-\varphi_{OPU}$ outperforms traditional methods both in accuracy and computation time.

\subsection{Comparing $GSA-\varphi_{OPU}$ and GIN-based model}\label{sec:vs_GIN}

In Fig \ref{fig:GCN}, we see that $GSA-\varphi_{OPU}$ with either RW sampling  ($k\geq4$) or uniform sampling ($k\geq5$) performs better than the GIN-based graph convolutional model of~\cite{GCN_powerful}. We do not report the computational time for GIN, as it is highly dependent on high-speed graphical processing units (GPUs) for training.



\begin{figure}
	%
	\begin{minipage}[b]{.48\linewidth}
		\centering
		\centerline{\includegraphics[width=4.3cm]{figs/phi_comparison.pdf}}
		%  \vspace{1.5cm}
	%	\centerline{(a) RF maps}\medskip
		\label{subfig:RF_maps}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.48\linewidth}
		\centering
		\centerline{\includegraphics[width=4.3cm]{figs/computational_comp.pdf}}
		%  \vspace{1.5cm}
%		\centerline{(b) Computation time}\medskip
	\end{minipage}
	%
	\caption{(left) Test accuracy versus $m$, for different maps  $\varphi$ in $GSA-\varphi$. (right) Computation time versus $k$ for $GSA-\varphi$ and the graphlet kernel. These figures are for $r=1.1$, $s=2000$, $m=5000$ and a Gaussian map variance $\sigma^2=0.01$.}
	\label{fig:diff_phi}
	%
\end{figure}


\begin{figure}
%
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.3cm]{figs/DD.pdf}}
%  \vspace{1.5cm}
 % \centerline{(a) D\&D }\medskip
  \label{subfig:RF_maps}
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.3cm]{figs/Reddit.pdf}}
%  \vspace{1.5cm}
  %\centerline{(b) Reddit-Binary}\medskip
\end{minipage}
%
\caption{$GSA-\varphi$ vs the graphlet kernel on real datasets. (left)~D\&D. (right) Reddit-Binary. With $s=4000$, and $k=7$.}
\label{fig:DD}
%
\end{figure}

\subsection{$GSA-\varphi_{OPU}$  on real datasets}\label{sec:DD_Reddit}
Fig \ref{fig:DD} shows the test accuracy of $GSA-\varphi_{OPU}$ versus $m$, for two real datasets. For each value of $m$ we conduct the experiment 3 times on D\&D and 4 times on Reddit-Binary dataset and take the average accuracy. For  D\&D, although results of the 3 experiments get more concentrated as $m$ increases, no clear and steady improvement in the average accuracy is observed. This might be accentuated by the fact that node features are ignored. However, this average is still better than the accuracy obtained by $GSA-\match$. For Reddit-Binary, the variance of experiments also decreases slightly with $m$. More importantly, the average accuracy is monotonically increasing, and outperforms  $GSA-\match$ for $m\geq5000$. 

% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak

\section{Conclusion}
\label{sec:Conclusion}

We proposed a generic framework that can deploy OPUs random features in graph classification, since OPUs compute such features  in  $\mathcal{O}(1)$ in both input/output dimensions. Then, we showed a concentration of the random embedding around the MMD metric. Our experiments showed that our algorithm is significantly faster than the graphlet kernel with graphlet sampling and  performs better while concentrating around the MMD metric. Moreover,  it outperformed a state-of-the-art graph convolutional network on graph classification.

 A major point left open to be analyzed is how to use our algorithm to classify graphs with node features. One promising possibility is to use our algorithm to generate features embeddings on the graph level, and then feed these embeddings with the nodes' features to a deep neural network. On the theoretical side, the properties of the MMD metric could be further analyzed on particular models of graphs to get a concentration with higher certainty. 




% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\small
\bibliography{strings,refs}
\normalsize
\include{app}
\end{document}

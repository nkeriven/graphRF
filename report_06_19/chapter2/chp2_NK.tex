\chapter{Fast graph kernel classifier based on optical random features }
\label{chapter:fast_algorithm}
Graphlet kernel  is a good method to solve graph classification problem but as we have seen in chapter \ref{chapter:background}, it suffers from a high computational cost. In this chapter, we take inspiration from graph sampling and averaging to propose a family of fast graph kernels that generalizes the graphlet kernel paradigm. We show how random features can be incorporated within the new framework to get a faster and competitive algorithm in graph classification. Finally, we describe how Optical Processing Units (OPUs) can be used to eliminate some significant computational cost altogether.

\section{Proposed algorithm}
We recall from chapter \ref{chapter:background} that the computational cost of graphlet kernel is $C_{gk}= O(\tbinom{v}{k} N_k C_k)$. As an attempt to lower this cost, using graph sampling we can compute an empirical approximation of $k$-spectrum vector so the new that cost becomes $C_{gk + gs}= O(C_S s N_k C_k)$. What changed is that $\tbinom{v}{k}$ is replaced with $C_S s$, but the question is whether that is enough or not. To answer this we recall that the minimum number of samples $s \sim N_k$ required to ensure some certainty sharply increases as the graphlet size increase. It is clear then that the number of graph samples is not the only bottleneck here but also the cost to compute $\varphi_k^{match}$, denoted by $C_{\varphi_k^{match}}=O(N_k C_k)$. So we propose to replace $\varphi^{match}_k$ with another user-defined function $\varphi:\phlet \mapsto\R^m$ and keep everything else as it is. We obtain a family of algorithms referred to as Graph Sampling and Averaging (GSA-$\varphi$), described in Alg. \ref{alg:GSA}.

\begin{algorithm}[H]\label{alg:GSA}
\DontPrintSemicolon
  \KwInput{2-Classes labelled graph dataset $\mathcal{X}=(\G_i,y_i)_{i=1,\ldots,n}$}
  \KwOutput{Trained model to classify graphs}
  \tools{Graph random sampler $S_k$, a function $\varphi$, linear classifier (ex. SVM) }\\
  \Hyp{k:graphlet size, $s$:\#graphlet samples per graph}\\
  %\KwData{Testing set $x$}
  %$\sum_{i=1}^{\infty} := 0$ \tcp*{this is a comment}
  %\tcc{}
  \Algo{\\}
  Random initialization of SVM weights\\
  \For{$\G_i$ in $\mathcal{X}$}{
  $\varphi_i=0$\\
  \For{$j=1:s$}{
  $F_{i,j}\gets S_k(\G_i)$\\
  $\varphi_i\gets \varphi_i +\frac{1}{s}\varphi(F_{i,j})$
  }
  }
  $\mathcal{D}_{\varphi}\gets (\varphi_i,Y_i)_{i=1,\ldots, n}$\\
  Train the linear classifier on the new vector-valued dataset $\mathcal{D}_{\varphi}$
\caption{Graph Sampling and Averaging (GSA-$\varphi$)}
\end{algorithm}

We note that within the new paradigm, the defined $\varphi$ does not necessarily respect the isomorphism between sampled subgraphs: if $\phi(F) = \phi(F')$ whenever $F \cong F'$, then we are in the framework of graphlet \emph{without} repetition, otherwise we are in the other case. However, it is possible to apply some preprocessing function $Q$ invariant by permutation before passing it to a randomized $\varphi$, and in this case isomorphism is respected without any condition on $\varphi$. An example of such function is $Q:\R^{k\times k}\mapsto \R^k, Q(F)=Sort(Eigenvalues(\mathbf{A}_F))$ is the sorted eigenvalues of the adjacency matrix.

\iffalse
\section{Using random features framework in our algorithm}
After we presented the generic algorithm, now we combine it with random features kernels.
Let's assume that we have a psd and shift-invariant graph kernel, as a recap, we know it can be written in the form:
\begin{equation}
\mathcal{K}(F,F')= \mathbb{E}_w, \xi_w(F)\xi_w(F')
\end{equation}
which gives that by defining:
\begin{equation}
\varphi(F) = \frac{1}{\sqrt{m}} ( \xi_{w_j}(F) )_{j=1}^m \in \mathbb{C}^m,~~~ m\in \mathbb{N}
\end{equation}
we can write:
\[
\mathcal{K}(F,F')\approx \varphi(F)^*\varphi(F')
\]
We first define the mean kernel methodology, which  allows to \emph{lift} a kernel from a domain $\mathcal{X}$ to a kernel on \emph{probability distributions} on $\mathcal{X}$. Given a base kernel $k$ and two probability distribution $P,Q$, it is defined as:
\begin{equation}
\label{eq:mean_kernel}
\mathcal{K}(P,Q) = \mathbb{E}_{x \sim P, y \sim Q} \mathcal{K}(x,y)
\end{equation}
methodology 

In other words, the mean kernel is just the expectation of the base kernel with respect to each term. The associated Euclidean metric is referred to by the  \emph{Maximum Mean Discrepancy (MMD)}, and is naturally defined as:
\begin{equation}\label{eq:MMD}
MMD(P,Q) = \sqrt{\mathcal{K}(P,P) + \mathcal{K}(Q,Q) - 2\mathcal{K}(P,Q)}
\end{equation}
It should be noticed here that $\mathcal{K}(P,P) = \mathbb{E}_{x \sim P, x' \sim P} \mathcal{K}(x,x') \neq \mathbb{E}_{x \sim P} \mathcal{K}(x,x)$.


%\section{Accelerate the algorithm with Optical random features}
\fi


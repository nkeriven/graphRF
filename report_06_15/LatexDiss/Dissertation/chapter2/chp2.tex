\chapter{Fast graph kernel classifier based on optical random features }
\label{chapter:fast_algorithm}
Graphlet kernel  is a good method to solve graph classification problem but as we have seen in chapter \ref{chapter:background}, it suffers from a high computational cost. In this chapter, we take inspiration from graph sampling and averaging to propose a family of fast graph kernels that generalizes the graphlet kernel paradigm. We show how random features can be incorporated within the new framework to get a faster and competitive algorithm in graph classification. Finally, we describe how Optical Processing Units (OPUs) can be used to eliminate some significant computational cost altogether.

\section{Proposed algorithm}
We recall from chapter \ref{chapter:background} that the computational cost of graphlet kernel is $C_{gk}= O(\tbinom{v}{k} N_k C_k)$. As an attempt to lower this cost, using graph sampling we can compute an empirical approximation of $k$-spectrum vector so the new that cost becomes $C_{gk + gs}= O(C_S s N_k C_k)$. What changed is that $\tbinom{v}{k}$ is replaced with $C_S s$, but the question is whether that is enough or not. We recall that the minimum number of samples $s \sim N_k$ required to ensure some certainty sharply increases as the graphlet size increase. It is clear then that the number of graph samples is not the only bottleneck here but also the cost to compute $\varphi_k^{match}$, denoted by $C_{\varphi_k^{match}}=O(N_k C_k)$.

So we propose to replace $\varphi^{match}_k$ with another user-defined function $\varphi:\phlet \mapsto\R^m$ and keep everything else as it is. We obtain a family of algorithms referred to as Graph Sampling and Averaging (GSA-$\varphi$), described in Alg. \ref{alg:GSA}.

\begin{algorithm}[H]\label{alg:GSA}
\DontPrintSemicolon
  \KwInput{2-Classes labelled graph dataset $\mathcal{X}=(\G_i,y_i)_{i=1,\ldots,n}$}
  \KwOutput{Trained model to classify graphs}
  \tools{Graph random sampler $S_k$, a function $\varphi$, linear classifier (ex. SVM) }\\
  \Hyp{k:graphlet size, $s$:\#graphlet samples per graph}\\
  %\KwData{Testing set $x$}
  %$\sum_{i=1}^{\infty} := 0$ \tcp*{this is a comment}
  %\tcc{}
  \Algo{\\}
  Random initialization of SVM weights\\
  \For{$\G_i$ in $\mathcal{X}$}{
  $\varphi_i=0$\\
  \For{$j=1:s$}{
  $F_{i,j}\gets S_k(\G_i)$\\
  $\varphi_i\gets \varphi_i +\frac{1}{s}\varphi(F_{i,j})$
  }
  }
  $\mathcal{D}_{\varphi}\gets (\varphi_i,Y_i)_{i=1,\ldots, n}$\\
  Train the linear classifier on the new vector-valued dataset $\mathcal{D}_{\varphi}$
\caption{Graph Sampling and Averaging (GSA-$\varphi$)}
\end{algorithm}

We note that within the new paradigm, the defined $\varphi$ does not necessarily respect the isomorphism between sampled subgraphs: if $\varphi(F) = \varphi(F')$ whenever $F \cong F'$, then we are in the framework of graphlet \emph{without} repetition, otherwise we are in the other case. As we will see, choosing a randomized $\varphi$ presents both theoretical and practical advantages, however it does not respect isomorphism. Nevertheless, it is possible to apply some preprocessing function $Q$ invariant by permutation before passing it to a randomized $\varphi$, and in this case isomorphism is respected without any condition on $\varphi$. An example of such function is $Q:\R^{k\times k}\mapsto \R^k, Q(F)=Sort(Eigenvalues(\mathbf{A}_F))$, that is, the sorted eigenvalues of the adjacency matrix.

\todoNK{A word on the generic computational cost of the method ? $O(C_S s C_\varphi)$, emphasizing that $C_\varphi$ is now the main focus in the rest (intuitively, trade off between computational cost and discriminative power)}

\section{Using random features framework in our algorithm}
After we presented the generic algorithm, now we combine it with random features kernels.
Let's assume that we have a psd and shift-invariant graph kernel, as a recap, we know it can be written in the form:
\begin{equation}
\mathcal{K}(F,F')= \mathbb{E}_w, \xi_w(F)\xi_w(F')
\end{equation}
which gives that by defining:
\begin{equation}
\varphi(F) = \frac{1}{\sqrt{m}} ( \xi_{w_j}(F) )_{j=1}^m \in \mathbb{C}^m,~~~ m\in \mathbb{N}
\end{equation}
we can write:
\[
\mathcal{K}(F,F')\approx \varphi(F)^*\varphi(F')
\]
In this point and based on such kernel, we define another one, called \emph{mean kernel} $\K_{mk}$, with presenting its corresponding metric, called \emph{Maximum Mean Discrepancy (MMD)}. Next, we show with the aid of concentration inequalities how using the random features map $\varphi$ of $\K$ in  our algorithm $GSA-\varphi$ will lead to an approximation of $\K_{mk}$ concentrated around its true value with high probability.

The mean kernel methodology allows to \emph{lift} a kernel from a domain $\phlet$ to a kernel on \emph{probability distributions} on $\phlet$. Given a base kernel $\K$ and two probability distribution $\mathcal{P},\mathcal{Q}$, it is defined as:
\begin{equation}
\label{eq:mean_kernel}
\mathcal{K}_{mk}(\mathcal{P},\mathcal{Q}) = \mathbb{E}_{x \sim \mathcal{P}, y \sim \mathcal{Q}} \mathcal{K}(x,y)
\end{equation}
In other words, the mean kernel is just the expectation of the base kernel with respect to each term. Mean kernel is associated to an Euclidean metric which is referred to by the  \emph{Maximum Mean Discrepancy (MMD)}, and is defined as:
\begin{equation}\label{eq:MMD}
MMD(\mathcal{P},\mathcal{Q}) = \sqrt{\mathcal{K}_{mk}(\mathcal{P},\mathcal{P}) + \mathcal{K}_{mk}(\mathcal{Q},\mathcal{Q}) - 2\mathcal{K}_{mk}(\mathcal{P},\mathcal{Q})}
\end{equation}
It should be noticed here that $\mathcal{K}_{mk}(\mathcal{P},\mathcal{P}) = \mathbb{E}_{x \sim \mathcal{P}, x' \sim \mathcal{P}} \mathcal{K}(x,x') \neq \mathbb{E}_{x \sim \mathcal{P}} \mathcal{K}(x,x)$.

\textbf{The link between the mean kernel and graphs} can be identified easily, since as we already pointed out that a graph $\G$ introduces a probability distribution $f_\G$ on the set of size-$k$ graphlets $\phlet$. Thus, for two graphs $\G,\G'$ , the mean kernel can be reformulated to:
\begin{equation}
\label{eq:mean_kernel_graphs}
\mathcal{K}_{mk}(\G,\G') = \mathcal{K}_{mk}(f_\G,f_\G')= \mathbb{E}_{F \sim S_k(\G), F' \sim S_k(\G')} \mathcal{K}(F,F')=\sum_{i,j}^{N_k}f_{\G,i}f_{\G',j}\mathcal{K}(\phlet,\phlet') 
\end{equation}
where $S_k$ is a random sampler that is compatible with the graphlet kernel random sampling method, such that uniform sampler but not random walk sampler.
.

.
.




\section{Accelerate the algorithm with Optical random features}


